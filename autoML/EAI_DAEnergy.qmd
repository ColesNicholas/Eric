---
title: "Eric_AutoML.qmd"
author: "Nicholas a. Coles"
format: html
editor_options: 
  chunk_output_type: console
---
# load libraries and data
```{r}
# for pulling google drive data
library(googledrive)
library(readr)

# time series autoML
library(h2o)
library(iForecast)
library(zoo)
library(slider)

# data processing
library(tidyverse)
library(readxl)
library(janitor)
library(cowplot)
library(gridExtra)

# idiosyncratic settings
theme_set(theme_classic())
options(scipen = 999) # no sci notation
set.seed(1967)
```

# pull and prepare data from Google Drive
```{r eval = F}
# download from drive
drive_download(
  as_id('1CZ9PwOr3TDqaV9tM2mv92KR0pl0W_Ghw'),
  path = "eia.input.xls",
  overwrite = TRUE)
```

```{r}
# create dataset
df <- 
  # open data
  read_xls('eai.energy.xls') %>% 
  
  # clean variable names and structures
  clean_names() %>% 
  mutate(date_time = as.Date(date_time)) %>% 
  
  # add some variable suggestions from human Eric
  mutate(al.go =  
           rto_combined_bidclose_load_forecast_average / rto_combined_reg_total_gen_offline_capacity_average_latest,
         ml.go = rto_combined_bidclose_load_forecast_maximum / rto_combined_reg_total_gen_offline_capacity_average_latest,
         anl.go = rto_combined_net_load_forecast_bid_close_average / rto_combined_reg_total_gen_offline_capacity_average_latest,
         mnl.go = rto_combined_net_load_forecast_bid_close_maximum / rto_combined_reg_total_gen_offline_capacity_average_latest,
         # dclm.al = a_b_c_a_bge_bidclose_load_forecast_b_pepco_bidclose_load_forecast_c_dominion_bidclose_load_forecast_maximum / rto_combined_bidclose_load_forecast_average,
         malm.ml = mid_atlantic_region_bidclose_load_forecast_maximum / rto_combined_bidclose_load_forecast_maximum
         ) %>% 
  # create squared load predictor (at request of human Nick)
  mutate(load_sqr = rto_combined_bidclose_load_forecast_average ^ 2)
```

# Specify the date we are targeting and (if necessary), input values
```{r}
target <- '2026-01-07'
```

# Add 1, 2, 7-day lags of y and all exogenous vars
```{r}
lags <- c(1, 2, 7)

add_lags <- 
  function(data, 
           target, 
           exog, 
           lags) {
    
    out <- data
    
    # y lags
    for (L in lags) {
      out[[paste0(target, "_lag_", L)]] <- lag(out[[target]], L)
    }
    
    # exogenous lags
    for (col in exog) {
      for (L in lags) {
        out[[paste0(col, "_lag_", L)]] <- dplyr::lag(out[[col]], L)
      }
    }
    out
  }

df_lag <- 
  add_lags(data = df, 
           target = "pjm_rto_daenergy_average", 
           exog = setdiff(colnames(df), 
                          c("date_time", "pjm_rto_daenergy_average",
                            "rto_combined_bidclose_load_forecast_average",
                            "aep_dayton_hub_dalmp_average", "n_illinois_hub_dalmp_average",
                            "dominion_hub_dalmp_average", "eastern_hub_dalmp_average")), 
           lags = lags)

rm(lags, add_lags)
```

# Train models
Create train, validation, and test datasets
```{r}
# Define length of validation and test windows
valid_days <- 50L
test_days  <- 50L

# indicate start of validateion and test windows
test_start  <- max(df_lag$date_time) - days(test_days)
valid_start <- test_start - days(valid_days)

# create splits (time-based)
train_df <- df_lag %>% filter(date_time <= valid_start)
valid_df <- df_lag %>% filter(date_time > valid_start & date_time <= test_start)
trainvalid_df <- df_lag %>% filter(date_time <= test_start)
test_df  <- df_lag %>% filter(date_time > test_start)

train_df$date_time %>% min()
train_df$date_time %>% max()

valid_df$date_time %>% min() 
valid_df$date_time %>% max()

trainvalid_df$date_time %>% min() 
trainvalid_df$date_time %>% max()

test_df$date_time %>% min()
test_df$date_time %>% max()

# Move to H2O
h2o.init()

train_h2o <- as.h2o(train_df)
valid_h2o <- as.h2o(valid_df)
trainvalid_h2o <- as.h2o(trainvalid_df)
test_h2o  <- as.h2o(test_df)
```

Specify predictors and outcome
```{r}
x <- setdiff(names(train_df), 
                          c("date_time", "western_hub_dalmp_average",
                            "rto_combined_bidclose_load_forecast_average",
                            "aep_dayton_hub_dalmp_average", "n_illinois_hub_dalmp_average",
                            "dominion_hub_dalmp_average", "eastern_hub_dalmp_average",
                            "western_hub_dalmp_average", "pjm_rto_daenergy_average"))

y <- "pjm_rto_daenergy_average"
```

Quick automl
```{r eval = F}
# fit 
automl <-
  h2o.automl(x = x,
             y = y,
             training_frame = trainvalid_h2o)

# evaluate
pred <- h2o.predict(automl, test_h2o)

results <- test_h2o %>% as.data.frame()

results$pred <- pred %>% as.vector()

results <- results %>%
  mutate(abs.err = abs(western_hub_dalmp_average - pred))

results$abs.err %>% median(na.rm = T)
```

Extract key parameters
```{r eval = F}
h2o.get_leaderboard(automl)
h2o.getModel('DeepLearning_grid_1_AutoML_1_20260103_195938_model_30')

param <- h2o.getModel('DeepLearning_grid_1_AutoML_1_20260103_195938_model_37')@parameters
```

Estimate prediction intervals
```{r}
# POINT
dl_point <- h2o.deeplearning(
  x = x, y = y,
  training_frame   = trainvalid_h2o,
  nfolds = 5,
  fold_assignment = "Modulo",
  overwrite_with_best_model = F,
  activation       = "RectifierWithDropout",
  hidden           = 50,
  epochs = 7232.229,
  seed = 1290607680847814765,
  epsilon = 0.000000001,
  input_dropout_ratio = 0.1,
  hidden_dropout_ratios = 0.4,
  stopping_rounds = 0,
  stopping_metric = "deviance",
  stopping_tolerance = 0.03809697,
  max_runtime_secs = 714.006,
  categorical_encoding = "OneHotInternal"
)

# 95
dl_95_lo <- h2o.deeplearning(
  x = x, y = y,
  training_frame   = trainvalid_h2o,
  nfolds = 5,
  fold_assignment = "Modulo",
  overwrite_with_best_model = F,
  activation       = "RectifierWithDropout",
  hidden           = 50,
  epochs = 7232.229,
  seed = 1290607680847814765,
  epsilon = 0.000000001,
  input_dropout_ratio = 0.1,
  hidden_dropout_ratios = 0.4,
  stopping_rounds = 0,
  stopping_metric = "deviance",
  stopping_tolerance = 0.03809697,
  max_runtime_secs = 714.006,
  categorical_encoding = "OneHotInternal",
  distribution     = "quantile",
  quantile_alpha   = 0.025
)

dl_95_hi <- h2o.deeplearning(
  x = x, y = y,
  training_frame   = trainvalid_h2o,
  nfolds = 5,
  fold_assignment = "Modulo",
  overwrite_with_best_model = F,
  activation       = "RectifierWithDropout",
  hidden           = 50,
  epochs = 7232.229,
  seed = 1290607680847814765,
  epsilon = 0.000000001,
  input_dropout_ratio = 0.1,
  hidden_dropout_ratios = 0.4,
  stopping_rounds = 0,
  stopping_metric = "deviance",
  stopping_tolerance = 0.03809697,
  max_runtime_secs = 714.006,
  categorical_encoding = "OneHotInternal",
  distribution     = "quantile",
  quantile_alpha   = 0.975
)

# 85
dl_85_lo <- h2o.deeplearning(
  x = x, y = y,
  training_frame   = trainvalid_h2o,
  nfolds = 5,
  fold_assignment = "Modulo",
  overwrite_with_best_model = F,
  activation       = "RectifierWithDropout",
  hidden           = 50,
  epochs = 7232.229,
  seed = 1290607680847814765,
  epsilon = 0.000000001,
  input_dropout_ratio = 0.1,
  hidden_dropout_ratios = 0.4,
  stopping_rounds = 0,
  stopping_metric = "deviance",
  stopping_tolerance = 0.03809697,
  max_runtime_secs = 714.006,
  categorical_encoding = "OneHotInternal",
  distribution     = "quantile",
  quantile_alpha   = 0.075
)

dl_85_hi <- h2o.deeplearning(
  x = x, y = y,
  training_frame   = trainvalid_h2o,
  nfolds = 5,
  fold_assignment = "Modulo",
  overwrite_with_best_model = F,
  activation       = "RectifierWithDropout",
  hidden           = 50,
  epochs = 7232.229,
  seed = 1290607680847814765,
  epsilon = 0.000000001,
  input_dropout_ratio = 0.1,
  hidden_dropout_ratios = 0.4,
  stopping_rounds = 0,
  stopping_metric = "deviance",
  stopping_tolerance = 0.03809697,
  max_runtime_secs = 714.006,
  categorical_encoding = "OneHotInternal",
  distribution     = "quantile",
  quantile_alpha   = 0.925
)

# 75
dl_75_lo <- h2o.deeplearning(
  x = x, y = y,
  training_frame   = trainvalid_h2o,
  nfolds = 5,
  fold_assignment = "Modulo",
  overwrite_with_best_model = F,
  activation       = "RectifierWithDropout",
  hidden           = 50,
  epochs = 7232.229,
  seed = 1290607680847814765,
  epsilon = 0.000000001,
  input_dropout_ratio = 0.1,
  hidden_dropout_ratios = 0.4,
  stopping_rounds = 0,
  stopping_metric = "deviance",
  stopping_tolerance = 0.03809697,
  max_runtime_secs = 714.006,
  categorical_encoding = "OneHotInternal",
  distribution     = "quantile",
  quantile_alpha   = 0.125
)

dl_75_hi <- h2o.deeplearning(
  x = x, y = y,
  training_frame   = trainvalid_h2o,
  nfolds = 5,
  fold_assignment = "Modulo",
  overwrite_with_best_model = F,
  activation       = "RectifierWithDropout",
  hidden           = 50,
  epochs = 7232.229,
  seed = 1290607680847814765,
  epsilon = 0.000000001,
  input_dropout_ratio = 0.1,
  hidden_dropout_ratios = 0.4,
  stopping_rounds = 0,
  stopping_metric = "deviance",
  stopping_tolerance = 0.03809697,
  max_runtime_secs = 714.006,
  categorical_encoding = "OneHotInternal",
  distribution     = "quantile",
  quantile_alpha   = 0.875
)

# 65
dl_65_lo <- h2o.deeplearning(
  x = x, y = y,
  training_frame   = trainvalid_h2o,
  nfolds = 5,
  fold_assignment = "Modulo",
  overwrite_with_best_model = F,
  activation       = "RectifierWithDropout",
  hidden           = 50,
  epochs = 7232.229,
  seed = 1290607680847814765,
  epsilon = 0.000000001,
  input_dropout_ratio = 0.1,
  hidden_dropout_ratios = 0.4,
  stopping_rounds = 0,
  stopping_metric = "deviance",
  stopping_tolerance = 0.03809697,
  max_runtime_secs = 714.006,
  categorical_encoding = "OneHotInternal",
  distribution     = "quantile",
  quantile_alpha   = 0.175
)

dl_65_hi <- h2o.deeplearning(
  x = x, y = y,
  training_frame   = trainvalid_h2o,
  nfolds = 5,
  fold_assignment = "Modulo",
  overwrite_with_best_model = F,
  activation       = "RectifierWithDropout",
  hidden           = 50,
  epochs = 7232.229,
  seed = 1290607680847814765,
  epsilon = 0.000000001,
  input_dropout_ratio = 0.1,
  hidden_dropout_ratios = 0.4,
  stopping_rounds = 0,
  stopping_metric = "deviance",
  stopping_tolerance = 0.03809697,
  max_runtime_secs = 714.006,
  categorical_encoding = "OneHotInternal",
  distribution     = "quantile",
  quantile_alpha   = 0.825)
```

```{r}
# Score the test set and combine results ---
pred_point <- h2o.predict(dl_point, test_h2o) %>% as.data.frame()

pred_95_lo <- h2o.predict(dl_95_lo, test_h2o) %>% as.data.frame()
pred_95_hi <- h2o.predict(dl_95_hi, test_h2o) %>% as.data.frame()

pred_85_lo <- h2o.predict(dl_85_lo, test_h2o) %>% as.data.frame()
pred_85_hi <- h2o.predict(dl_85_hi, test_h2o) %>% as.data.frame()

pred_75_lo <- h2o.predict(dl_75_lo, test_h2o) %>% as.data.frame()
pred_75_hi <- h2o.predict(dl_75_hi, test_h2o) %>% as.data.frame()

pred_65_lo <- h2o.predict(dl_65_lo, test_h2o) %>% as.data.frame()
pred_65_hi <- h2o.predict(dl_65_hi, test_h2o) %>% as.data.frame()

results <- test_df %>%
  select(date_time, y) %>%
  bind_cols(
    yhat   = pred_point$predict,
    lo95   = pred_95_lo$predict,
    hi95   = pred_95_hi$predict,
    lo85   = pred_85_lo$predict,
    hi85   = pred_85_hi$predict,
    lo75   = pred_75_lo$predict,
    hi75   = pred_75_hi$predict,
    lo65   = pred_65_lo$predict,
    hi65   = pred_65_hi$predict
  )
```

# Generate report
## Time series

### Notes

This needs to have median accuracy posted.

This also needs to have a legend
```{r}
ts <- 
  ggplot(results, aes(x = date_time)) +
  
  # plot prediction w/ band
  geom_ribbon(aes(ymin = lo95, 
                  ymax = hi95), 
              fill = "grey95") +
  geom_ribbon(aes(ymin = lo85, 
                  ymax = hi85), 
              fill = "grey85") +
  geom_ribbon(aes(ymin = lo75, 
                  ymax = hi75), 
              fill = "grey75") +
  geom_ribbon(aes(ymin = lo65, 
                  ymax = hi65), 
              fill = "grey65") +
  geom_line(aes(y = yhat),
            linewidth = 1,
            linetype = 'dotted') +
  
  # plot actual
  geom_point(aes(y = pjm_rto_daenergy_average), 
             size = 1.2, 
             alpha = 0.7) +
  geom_line(aes(y = pjm_rto_daenergy_average), 
            linewidth = 1) +
  ylab('DA-LMP') +
  xlab('date')+
  scale_x_date(date_breaks = "1 day", date_labels = "%b %d") +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```

## Prediction bands
```{r}
# target
pb.t.input <-
  results %>% 
  filter(date_time == target) %>% 
  pivot_longer(lo95 : hi65) %>% 
  extract(
    col   = name,                 # column to split
    into  = c("bound", "width"),     # new columns: hi/lo and numeric width
    regex = "^(hi|lo)(\\d+)$",       # capture 'hi' or 'lo' then digits
    convert = TRUE                   # converts width to integer
  ) %>% 
  pivot_wider(names_from = 'bound',
              values_from = 'value')

pb.tp1.input <-
  results %>% 
  filter(date_time == as.Date(target) + days(1)) %>% 
  pivot_longer(lo95 : hi65) %>% 
  extract(
    col   = name,                 # column to split
    into  = c("bound", "width"),     # new columns: hi/lo and numeric width
    regex = "^(hi|lo)(\\d+)$",       # capture 'hi' or 'lo' then digits
    convert = TRUE                   # converts width to integer
  ) %>% 
  pivot_wider(names_from = 'bound',
              values_from = 'value')

pb.tp2.input <-
  results %>% 
  filter(date_time == as.Date(target) + days(2)) %>% 
  pivot_longer(lo95 : hi65) %>% 
  extract(
    col   = name,                 # column to split
    into  = c("bound", "width"),     # new columns: hi/lo and numeric width
    regex = "^(hi|lo)(\\d+)$",       # capture 'hi' or 'lo' then digits
    convert = TRUE                   # converts width to integer
  ) %>% 
  pivot_wider(names_from = 'bound',
              values_from = 'value')

pb.tp3.input <-
  results %>% 
  filter(date_time == as.Date(target) + days(3)) %>% 
  pivot_longer(lo95 : hi65) %>% 
  extract(
    col   = name,                 # column to split
    into  = c("bound", "width"),     # new columns: hi/lo and numeric width
    regex = "^(hi|lo)(\\d+)$",       # capture 'hi' or 'lo' then digits
    convert = TRUE                   # converts width to integer
  ) %>% 
  pivot_wider(names_from = 'bound',
              values_from = 'value')

pb.tp4.input <-
  results %>% 
  filter(date_time == as.Date(target) + days(4)) %>% 
  pivot_longer(lo95 : hi65) %>% 
  extract(
    col   = name,                 # column to split
    into  = c("bound", "width"),     # new columns: hi/lo and numeric width
    regex = "^(hi|lo)(\\d+)$",       # capture 'hi' or 'lo' then digits
    convert = TRUE                   # converts width to integer
  ) %>% 
  pivot_wider(names_from = 'bound',
              values_from = 'value')

pb.tp5.input <-
  results %>% 
  filter(date_time == as.Date(target) + days(5)) %>% 
  pivot_longer(lo95 : hi65) %>% 
  extract(
    col   = name,                 # column to split
    into  = c("bound", "width"),     # new columns: hi/lo and numeric width
    regex = "^(hi|lo)(\\d+)$",       # capture 'hi' or 'lo' then digits
    convert = TRUE                   # converts width to integer
  ) %>% 
  pivot_wider(names_from = 'bound',
              values_from = 'value')
```

## Prediction
```{r}
pb.t <- 
  ggplot(data = pb.t.input,
         aes(x = width)) +
  geom_ribbon(aes(ymin = lo, 
                  ymax = hi), 
              fill = "grey95") +
  geom_hline(yintercept = pb.t.input$yhat[1],
             linetype = 'dotted') +
  scale_x_continuous(breaks = c(65, 75, 85, 95)) +
  labs(x = 'prediction interval width',
       y = 'predicted DA-LMP',
       title = paste(pb.t.input$date_time[1],
                     "prediction : $",
                     pb.t.input$yhat[1] %>% round(3),
                     " / MWh"))
```

```{r}
pb.tp1 <- 
  ggplot(data = pb.tp1.input,
         aes(x = width)) +
  geom_ribbon(aes(ymin = lo, 
                  ymax = hi), 
              fill = "grey95") +
  geom_hline(yintercept = pb.tp1.input$yhat[1],
             linetype = 'dotted') +
  scale_x_continuous(breaks = c(65, 75, 85, 95)) +
  labs(x = 'prediction interval width',
       y = 'predicted DA-LMP',
       title = paste(pb.tp1.input$date_time[1],
                     "prediction : $",
                     pb.tp1.input$yhat[1] %>% round(2),
                     " / MWh"))
```

```{r}
pb.tp2 <- 
  ggplot(data = pb.tp2.input,
         aes(x = width)) +
  geom_ribbon(aes(ymin = lo, 
                  ymax = hi), 
              fill = "grey95") +
  geom_hline(yintercept = pb.tp2.input$yhat[1],
             linetype = 'dotted') +
  scale_x_continuous(breaks = c(65, 75, 85, 95)) +
  labs(x = 'prediction interval width',
       y = 'predicted DA-LMP',
       title = paste(pb.tp2.input$date_time[1],
                     "prediction : $",
                     pb.tp2.input$yhat[1] %>% round(2),
                     " / MWh"))
```

```{r}
pb.tp3 <- 
  ggplot(data = pb.tp3.input,
         aes(x = width)) +
  geom_ribbon(aes(ymin = lo, 
                  ymax = hi), 
              fill = "grey95") +
  geom_hline(yintercept = pb.tp3.input$yhat[1],
             linetype = 'dotted') +
  scale_x_continuous(breaks = c(65, 75, 85, 95)) +
  labs(x = 'prediction interval width',
       y = 'predicted DA-LMP',
       title = paste(pb.tp3.input$date_time[1],
                     "prediction : $",
                     pb.tp3.input$yhat[1] %>% round(2),
                     " / MWh"))
```

```{r}
pb.tp4 <- 
  ggplot(data = pb.tp4.input,
         aes(x = width)) +
  geom_ribbon(aes(ymin = lo, 
                  ymax = hi), 
              fill = "grey95") +
  geom_hline(yintercept = pb.tp4.input$yhat[1],
             linetype = 'dotted') +
  scale_x_continuous(breaks = c(65, 75, 85, 95)) +
  labs(x = 'prediction interval width',
       y = 'predicted DA-LMP',
       title = paste(pb.tp4.input$date_time[1],
                     "prediction : $",
                     pb.tp4.input$yhat[1] %>% round(2),
                     " / MWh"))
```

```{r}
pb.tp5 <- 
  ggplot(data = pb.tp5.input,
         aes(x = width)) +
  geom_ribbon(aes(ymin = lo, 
                  ymax = hi), 
              fill = "grey95") +
  geom_hline(yintercept = pb.tp5.input$yhat[1],
             linetype = 'dotted') +
  scale_x_continuous(breaks = c(65, 75, 85, 95)) +
  labs(x = 'prediction interval width',
       y = 'predicted DA-LMP',
       title = paste(pb.tp5.input$date_time[1],
                     "prediction : $",
                     pb.tp5.input$yhat[1] %>% round(2),
                     " / MWh"))
```

## Summary
Prepare summary
```{r}
summary <-
  results %>% 
  pivot_longer(lo95 : hi65) %>% 
  extract(
    col   = name,                 # column to split
    into  = c("bound", "width"),     # new columns: hi/lo and numeric width
    regex = "^(hi|lo)(\\d+)$",       # capture 'hi' or 'lo' then digits
    convert = TRUE                   # converts width to integer
  ) %>% 
  pivot_wider(names_from = 'bound',
              values_from = 'value') %>% 
  mutate(abs.err = abs(pjm_rto_daenergy_average - yhat),
         hit = 
           if_else(pjm_rto_daenergy_average > lo & pjm_rto_daenergy_average < hi,
                   true = 1,
                   false = 0))

# last checked: $5.21 ($4.59 with new variables)
summary$abs.err %>% median(na.rm = T)

summary2 <- summary %>% 
  group_by(width) %>% 
  summarise(accuracy = mean(hit,
                            na.rm = T),
            accuracy = round(accuracy, 3)) %>% 
  rename('prediction interval width' = 'width',
         'tested accuracy' = 'accuracy')
```

s.t
```{r}
s.t <- summary %>% 
  filter(as.character(date_time) == target) %>% 
  select(width, lo, hi) %>% 
  mutate(lo = round(lo, 2),
         hi = round(hi, 2)) %>% 
  rename('prediction interval width' = 'width') %>% 
  right_join(summary2,
             by = 'prediction interval width') %>% 
  select(`prediction interval width`, `tested accuracy`, lo, hi) %>% 
  tableGrob(rows = NULL)
```

```{r}
s.t1 <- summary %>% 
  filter(as.character(date_time) == 
           as.Date(target) + days(1)) %>% 
  select(width, lo, hi) %>% 
  mutate(lo = round(lo, 2),
         hi = round(hi, 2)) %>% 
  rename('prediction interval width' = 'width') %>% 
  right_join(summary2,
             by = 'prediction interval width') %>% 
  select(`prediction interval width`, `tested accuracy`, lo, hi) %>% 
  tableGrob(rows = NULL)
```

```{r}
s.t2 <- summary %>% 
  filter(as.character(date_time) == 
           as.Date(target) + days(2)) %>% 
  select(width, lo, hi) %>% 
  mutate(lo = round(lo, 2),
         hi = round(hi, 2)) %>% 
  rename('prediction interval width' = 'width') %>% 
  right_join(summary2,
             by = 'prediction interval width') %>% 
  select(`prediction interval width`, `tested accuracy`, lo, hi) %>% 
  tableGrob(rows = NULL)
```

```{r}
s.t3 <- summary %>% 
  filter(as.character(date_time) == 
           as.Date(target) + days(3)) %>% 
  select(width, lo, hi) %>% 
  mutate(lo = round(lo, 2),
         hi = round(hi, 2)) %>% 
  rename('prediction interval width' = 'width') %>% 
  right_join(summary2,
             by = 'prediction interval width') %>% 
  select(`prediction interval width`, `tested accuracy`, lo, hi) %>% 
  tableGrob(rows = NULL)
```

```{r}
s.t4 <- summary %>% 
  filter(as.character(date_time) == 
           as.Date(target) + days(4)) %>% 
  select(width, lo, hi) %>% 
  mutate(lo = round(lo, 2),
         hi = round(hi, 2)) %>% 
  rename('prediction interval width' = 'width') %>% 
  right_join(summary2,
             by = 'prediction interval width') %>% 
  select(`prediction interval width`, `tested accuracy`, lo, hi) %>% 
  tableGrob(rows = NULL)
```

```{r}
s.t5 <- summary %>% 
  filter(as.character(date_time) == 
           as.Date(target) + days(5)) %>% 
  select(width, lo, hi) %>% 
  mutate(lo = round(lo, 2),
         hi = round(hi, 2)) %>% 
  rename('prediction interval width' = 'width') %>% 
  right_join(summary2,
             by = 'prediction interval width') %>% 
  select(`prediction interval width`, `tested accuracy`, lo, hi) %>% 
  tableGrob(rows = NULL)
```

## Final
```{r}
plot_grid(ts, 
          plot_grid(pb.t, s.t, ncol = 2),
          plot_grid(pb.tp1, s.t1, ncol = 2),
          plot_grid(pb.tp2, s.t2, ncol = 2),
          plot_grid(pb.tp3, s.t3, ncol = 2),
          plot_grid(pb.tp4, s.t4, ncol = 2),
          plot_grid(pb.tp5, s.t5, ncol = 2),
          nrow = 7,
          rel_heights = c(1, rep(.7, 6)))
```


# New idea: Expected value curves
```{r}
ev.t.input <-
  results %>% 
  filter(date_time == target)

quantile <- 
  c(0.025, 0.075, 
    0.125, 0.175, 
    0.50, 
    0.825, 0.875, 
    0.925, 0.975)

predicted.value <- 
  c(ev.t.input$lo95, ev.t.input$lo85,
    ev.t.input$lo75, ev.t.input$lo65,
    ev.t.input$yhat,
    ev.t.input$hi65, ev.t.input$hi75,
    ev.t.input$hi85, ev.t.input$hi95)

ggplot(data = cbind(p = p,
                    q = q),
       aes(x = p, y = q)) +
  geom_point() +
  geom_line() +
  xlab('probability') +
  ylab('price') +
  geom_smooth(se = F)

# Smooth spline fit
fit <- smooth.spline(x = quantile, y = predicted.value, 
                     spar = 0.25, all.knots = TRUE)


pred <- 
  predict(fit,
          x = seq(0.01, 
                  0.99, 
                  by = .01)) %>% 
  as.data.frame()

deriv_fit <- 
  predict(fit,
          x = seq(0.01, 
                  0.99, 
                  by = .01),
          deriv = 1) %>% 
  as.data.frame()

density <- 1 / deriv_fit$y

density_df <- data.frame(
  predicted_value = pred$y,
  density = density
)

ggplot(density_df, aes(x = predicted_value, y = density)) +
  geom_smooth(se = F) +
  xlab('predicted price') +
  ylab('probablity of occuring')
  
  labs(title = "Implied Price Density",
       x = "Predicted Price",
       y = "Density") +
  theme_minimal()




 # build a callable quantile function Q(u)
Q <- function(u) {
  # ensure u within observed range
  predict(fit, u)$y
}


# 5) Evaluate on a fine grid and compute approximate PDF
u <- seq(0.001, 0.999, length.out = 1000)  # probability grid
  value <- Q(u)                               # quantile values

# Numerical derivative: Q'(u) via central differences
du <- c(diff(u), NA)
dQ <- c(diff(value), NA)
# To avoid NA at the last point, copy previous step
du[length(du)] <- du[length(du)-1]
dQ[length(dQ)] <- dQ[length(dQ)-1]

# PDF estimate: f(v) = 1 / Q'(u) where v = Q(u)
pdf_est <- 1 / (dQ / du)

# Build tidy data frame: probability (u), value (quantile), cdf, pdf
dist_df <- data.frame(
  prob = u,
  value = value,
  cdf = u,
  pdf = pdf_est
)


# Evaluate on a fine grid and compute approximate PDF
u <- seq(0.001, 0.999, length.out = 1000)  # probability grid
value <- Q(u)                               # quantile values



# Turn into a function for integration
curve_fn <- function(x) predict(fit, x)$y

integrate(f = curve_fn,
          lower = 0,
          upper = 1)$value

u <- seq(.001, .999,
         length.out = 1000)
y <- curve_fn(u)

dist.df <- data.frame(probability = u,
                      value = y)

ggplot(dist.df,
       aes(x = probability,
           y = value)) +
  geom_point()

bids <- seq(0, 200, by = .25)

expected_profit <-
  function(bid){
    integrate(f = curve_fn,
          lower = 0,
          upper = 1)$value
  }

for (b in bids){
  tmp <- dist.df
  
  tmp$bid = b
  
  tmp <- tmp %>% 
    rowwise() %>% 
    mutate(error = value - bid,
           ev = error * probability) %>% 
    ungroup()
  
  return(mean(tmp$ev,
              na.rm = T))
}

ev.curve.input <-
  lapply(bids,
       function(b){
         tmp <- dist.df
         
         tmp$bid = b
         
         tmp <- tmp %>% 
           rowwise() %>% 
           mutate(error = value - bid,
                  ev = error * probability) %>% 
           ungroup()
         
         c(bid = b,
           ev = mean(tmp$ev,
                     na.rm = T)) %>% 
           return()
         
       })

ev.curve <- ev.curve.input %>% bind_rows()

ggplot(ev.curve,
       aes(x = ev,
           y = bid)) +
  geom_point()


ggplot(tmp,
       aes(probability,
           value)) +
  geom_point()

ggplot(tmp,
       aes(value,
           ev)) +
  geom_point(alpha = .05) +
  geom_smooth()
  
```

