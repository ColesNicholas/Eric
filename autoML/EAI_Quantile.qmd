---
title: "Eric_AutoML.qmd"
author: "Nicholas a. Coles"
format: html
editor_options: 
  chunk_output_type: console
---
# load libraries and data
```{r}
# for pulling google drive data
library(googledrive)
library(readr)

# time series autoML
library(h2o)
library(iForecast)
library(zoo)
library(slider)

# data processing
library(tidyverse)
library(readxl)
library(janitor)
library(cowplot)

# idiosyncratic settings
theme_set(theme_classic())
options(scipen = 999) # no sci notation
set.seed(1967)
```

# pull and prepare data from Google Drive
```{r}
# download from drive
drive_download(
  as_id('18wpeytHVNN-PmJrM18DkFoWc8CnbUgE5'),
  path = "eia.input.xls",
  overwrite = TRUE)

# create dataset
df <- 
  # open data
  read_xls('eia.input.xls') %>% 
  
  # if gas price missing, use rolling average of past three days
  mutate(
    `Henry (GASPRICE) Average` = 
      if_else(is.na(`Henry (GASPRICE) Average`),
              slide_dbl(
                `Henry (GASPRICE) Average`,
                mean,
                .before = 3,
                .after = -1,
                na.rm = TRUE),
              `Henry (GASPRICE) Average`),
    
    `TETCO-M3 (GASPRICE) Average` = 
      if_else(is.na(`TETCO-M3 (GASPRICE) Average`),
              slide_dbl(
                `TETCO-M3 (GASPRICE) Average`,
                mean,
                .before = 3,
                .after = -1,
                na.rm = TRUE),
              `TETCO-M3 (GASPRICE) Average`),
  ) %>% 
  
  # clean variable names and structures
  clean_names() %>% 
  mutate(date_time = as.Date(date_time)) %>% 
  
  # add some variable suggestions from human Eric
  mutate(al.go =  
           rto_combined_bidclose_load_forecast_average / rto_combined_reg_total_gen_offline_capacity_average_latest,
         ml.go = rto_combined_bidclose_load_forecast_maximum / rto_combined_reg_total_gen_offline_capacity_average_latest,
         anl.go = rto_combined_net_load_forecast_bid_close_average / rto_combined_reg_total_gen_offline_capacity_average_latest,
         mnl.go = rto_combined_net_load_forecast_bid_close_maximum / rto_combined_reg_total_gen_offline_capacity_average_latest,
         dclm.al = a_b_c_a_bge_bidclose_load_forecast_b_pepco_bidclose_load_forecast_c_dominion_bidclose_load_forecast_maximum / rto_combined_bidclose_load_forecast_average,
         malm.ml = mid_atlantic_region_bidclose_load_forecast_maximum / rto_combined_bidclose_load_forecast_maximum
         ) %>% 
  # create squared load predictor (at request of human Nick)
  mutate(load_sqr = rto_combined_bidclose_load_forecast_average ^ 2)
```

# Specify the date we are targeting and (if necessary), input values
```{r}
target <- '2026-01-01'
```

# Add 1, 2, 7-day lags of y and all exogenous vars
```{r}
lags <- c(1, 2, 7)

add_lags <- 
  function(data, 
           target, 
           exog, 
           lags) {
    
    out <- data
    
    # y lags
    for (L in lags) {
      out[[paste0(target, "_lag_", L)]] <- lag(out[[target]], L)
    }
    
    # exogenous lags
    for (col in exog) {
      for (L in lags) {
        out[[paste0(col, "_lag_", L)]] <- dplyr::lag(out[[col]], L)
      }
    }
    out
  }

df_lag <- 
  add_lags(data = df, 
           target = "western_hub_dalmp_average", 
           exog = setdiff(colnames(df), 
                          c("date_time", "western_hub_dalmp_average",
                            "rto_combined_bidclose_load_forecast_average")), 
           lags = lags)

```

# Train model
```{r}
# 2) Define 50-day validation and 50-day test windows
valid_days <- 50L
test_days  <- 50L

test_start  <- max(df_lag$date_time) - days(test_days)
valid_start <- test_start - days(valid_days)

# 3) Create splits (time-based)
train_df <- df_lag %>% filter(date_time <= valid_start)
valid_df <- df_lag %>% filter(date_time > valid_start & date_time <= test_start)
test_df  <- df_lag %>% filter(date_time > test_start)

train_df$date_time %>% min()
train_df$date_time %>% max()

valid_df$date_time %>% min() 
valid_df$date_time %>% max()

test_df$date_time %>% min()
test_df$date_time %>% max()

# Move to H2O
h2o.init(max_mem_size = "8g")

train_h2o <- as.h2o(train_df)
valid_h2o <- as.h2o(valid_df)
test_h2o  <- as.h2o(test_df)

# features: everything except date and the original y
x <- setdiff(names(train_df), c("date_time", "western_hub_dalmp_average"))
y <- "western_hub_dalmp_average"


# Point forecast model (Gaussian GBM) ---
# gbm_point <- h2o.gbm(
#   x = x, y = y,
#   training_frame   = train_h2o,
#   distribution     = "gaussian",
#   ntrees = 500,
#   seed = 1967
# )


# Mean/central tendency
dl_point <- h2o.deeplearning(
  x = x, y = y,
  training_frame   = train_h2o,
  validation_frame = valid_h2o,      # enables early stopping
  distribution     = "gaussian",     # point forecast loss
  hidden           = c(64, 64),      # small network
  activation       = "RectifierWithDropout",
  input_dropout_ratio   = 0.05,
  hidden_dropout_ratios = c(0.2, 0.2),
  l2 = 1e-4,
  epochs = 100,                      # stop early if no val. improvement
  stopping_rounds   = 5,
  stopping_metric   = "RMSE",
  stopping_tolerance = 1e-3,
  seed = 1967
)

# Lower bound: 2.5% quantile
dl_lo <- h2o.deeplearning(
  x = x, y = y,
  training_frame   = train_h2o,
  validation_frame = valid_h2o,
  distribution     = "quantile",
  quantile_alpha   = 0.025,          # lower tail
  hidden           = c(64, 64),
  activation       = "RectifierWithDropout",
  input_dropout_ratio   = 0.05,
  hidden_dropout_ratios = c(0.2, 0.2),
  l2 = 1e-4,
  epochs = 100,
  stopping_rounds   = 5,
  stopping_metric   = "MAE",         # aligns with quantile loss
  stopping_tolerance = 1e-3,
  seed = 1967
)

# Upper bound: 97.5% quantile
dl_hi <- h2o.deeplearning(
  x = x, y = y,
  training_frame   = train_h2o,
  validation_frame = valid_h2o,
  distribution     = "quantile",
  quantile_alpha   = 0.975,          # upper tail
  hidden           = c(64, 64),
  activation       = "RectifierWithDropout",
  input_dropout_ratio   = 0.05,
  hidden_dropout_ratios = c(0.2, 0.2),
  l2 = 1e-4,
  epochs = 100,
  stopping_rounds   = 5,
  stopping_metric   = "MAE",
  stopping_tolerance = 1e-3,
  seed = 1967
)


# Lower bound: 2.5% quantile
# gbm_lo <- h2o.gbm(
#   x = x, y = y,
#   training_frame   = train_h2o,
#   distribution     = "quantile",  # Quantile loss
#   quantile_alpha   = 0.025,       # 2.5% quantile
#   ntrees = 500,
#   seed = 1967
# )

# Upper bound: 97.5% quantile
# gbm_hi <- h2o.gbm(
#   x = x, y = y,
#   training_frame   = train_h2o,
#   validation_frame = test_h2o,
#   distribution     = "quantile",
#   quantile_alpha   = 0.975,       # 97.5% quantile
#   ntrees = 1200,
#   seed = 1967
# )


# Score the test set and combine results ---
pred_point <- h2o.predict(dl_point, test_h2o) %>% as.data.frame()
pred_lo    <- h2o.predict(dl_lo,    test_h2o) %>% as.data.frame()
pred_hi    <- h2o.predict(dl_hi,    test_h2o) %>% as.data.frame()

results <- test_df %>%
  select(date_time, y) %>%
  bind_cols(
    yhat   = pred_point$predict,
    lo95   = pred_lo$predict,
    hi95   = pred_hi$predict
  ) %>% 
  mutate(hit = 
           if_else(western_hub_dalmp_average < hi95 &
                     western_hub_dalmp_average > lo95,
                   'yes',
                   'no'))

results$hit %>% table %>% prop.table

#View(results)

# Optional: plot
ggplot(results, aes(x = date_time)) +
  geom_ribbon(aes(ymin = lo95, ymax = hi95), fill = "skyblue", alpha = 0.25) +
  geom_line(aes(y = yhat), color = "steelblue", linewidth = 1) +
  geom_point(aes(y = western_hub_dalmp_average,
                 color = hit), 
             size = 1.2, 
             alpha = 0.7) +
  ylab('DA-LMP') +
  xlab('date')+
  scale_x_date(date_breaks = "1 day", date_labels = "%b %d") +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

```

# Inspect model behavior
```{r}
# feature reliance
h2o.varimp_heatmap(object = automl$modelsUsed, 
                   top_n = 10, 
                   num_of_features = 20)
```

```{r}
# compile test data it's never seen
testData2 <- 
  window(automl$data,
         start = train.end,
         end = end(automl$data))

# create evaluation function
EvalFun <- function(m){
  
  # predict out of sample using favorite model
  preds <- 
    h2o.predict(m, 
                as.h2o(testData2)) %>% 
    as.data.frame()
  
  # evaluate out-of-sample error
  eval <-
    # join dataset
    cbind(testData2, 
          preds) %>% 
    as.data.frame() %>% 
    rownames_to_column('time') %>%
    
    # calculate error
    rowwise() %>% 
    mutate(abs_error = abs(y - predict),
           per_error = 
             abs((y - predict) / y) * 100) %>% 
    ungroup()
  
  return(eval)
}

# evaluate, remove target, and save
m.eval <-
  automl$output %>% 
  EvalFun %>% 
  mutate(reg = 
           ntile(x = sqrt(load_sqr_L0),
                 n = 4))

df.target <- 
  m.eval %>% 
  filter(time == target)

m.eval <- 
  m.eval %>% 
  filter(time != target)

# medAE (medium absolute error, or typical wrongness): $6.15
# medAbsolute Percentage Error (10.55%)
median(m.eval$abs_error)
median(m.eval$per_error)

# regime
metrics <-
  m.eval %>% 
  group_by(reg) %>% 
  summarise(n = n(),
            min.l = 
              min(sqrt(load_sqr_L0)), 
            max.l = 
              max(sqrt(load_sqr_L0)), 
            medae = 
              median(abs_error), 
            medape = 
              median(per_error)
            )

# insight: especially tight at low prices
plot(m.eval$rto_combined_bidclose_load_forecast_average_L0,
     m.eval$per_error)
```

Identify target prediction
```{r}
# what's the regime?
df.target$rto_combined_bidclose_load_forecast_average_L0
df.target$reg

# how have I been doing?  
paste0("EAI predicts that the DA-LMP for ",
       target,
       " will be: $",
       round(df.target$predict,
             3))

paste("In the past",
      metrics[metrics$reg == df.target$reg, ]$n,
      "tests on days with similar projected load regime,",
      "its prediction have had a median absolute error of $",
      metrics[metrics$reg == df.target$reg, ]$medae %>% 
        round(2),
      "($", metrics[metrics$reg == df.target$reg, ]$medape %>% 
        round(2), "%)")
df.target$predict
```
